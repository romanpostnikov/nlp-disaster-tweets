{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "nlp-disaster-tweets(spacy, tfidf).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyRJARy7R7JM"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "AxfV42KFSSJK",
        "outputId": "73c2859f-70e1-4026-fc63-5b518ce9a5c0"
      },
      "source": [
        "dataset = pd.read_csv(\"train.csv\")\n",
        "dataset.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ...                                               text target\n",
              "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
              "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
              "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
              "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
              "4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DRfHY93SXUW"
      },
      "source": [
        "# Plan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq0PRl02SVzy"
      },
      "source": [
        "So I will try to compose a few combinations of data & models to find the best one and use it as a final one. Those combinations are:\n",
        "* Data with no changes (only tweet text used);\n",
        "  * Use this data to build mostly used sklearn ML models and one DL model:\n",
        "    * Logistic Regression;\n",
        "    * K-neighbors;\n",
        "    * Decision Tree;\n",
        "    * Ensemble;\n",
        "    * ANN;\n",
        "* Data with cleaned up text in tweets (removed symbols, stop-words and stemmed text);\n",
        "  * Use this data to build mostly used sklearn ML models and one DL model (same models as in previous chapter);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWZVaDdPG8il"
      },
      "source": [
        "# def concatenate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eVGg8w-dRSX"
      },
      "source": [
        "# Let's first create the function which will concatenate text from tweets with \n",
        "# location and keyword fields.\n",
        "\n",
        "import spacy\n",
        "\n",
        "def concatenate(dataset):\n",
        "  corpus = []\n",
        "  for i in range(0, len(dataset)):\n",
        "    entry = dataset['text'][i]\n",
        "    if not str(dataset['keyword'][i]) == 'nan':\n",
        "      entry += str(dataset['keyword'][i])\n",
        "    if not str(dataset['location'][i]) == 'nan':\n",
        "      entry += str(dataset['location'][i])\n",
        "\n",
        "    corpus.append(entry)\n",
        "\n",
        "  return corpus"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2giJbdUw-NVx"
      },
      "source": [
        "# def lemmatize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B7efgF2froB"
      },
      "source": [
        "# Now let's create the function which will lemmatize text from tweets with\n",
        "\n",
        "def lemmatize(texts):\n",
        "  nlp = spacy.load('en')\n",
        "  tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
        "  corpus = []\n",
        "  for text in texts:\n",
        "    tokens = tokenizer(text)\n",
        "    tokens = [token.lemma_.lower() for token in tokens]\n",
        "    tokens = ' '.join(tokens)\n",
        "    corpus.append(tokens)\n",
        "\n",
        "  return corpus"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRC9gG8A-QzM"
      },
      "source": [
        "# def remove_stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItMyKymUl_lX"
      },
      "source": [
        "def remove_stopwords(texts):\n",
        "  nlp = spacy.load('en')\n",
        "  tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
        "  corpus = []\n",
        "  for text in texts:\n",
        "    tokens = tokenizer(text)\n",
        "    tokens = [token.text for token in tokens if not token.is_stop]\n",
        "    tokens = ' '.join(tokens)\n",
        "    corpus.append(tokens)\n",
        "\n",
        "  return corpus"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn24EiNd-U0m"
      },
      "source": [
        "# def remove_punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV8cEr18mHve"
      },
      "source": [
        "def remove_punctuation(texts):\n",
        "  nlp = spacy.load('en')\n",
        "  tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
        "  corpus = []\n",
        "  for text in texts:\n",
        "    tokens = tokenizer(text)\n",
        "    tokens = [token.text for token in tokens if not token.is_punct]\n",
        "    tokens = ' '.join(tokens)\n",
        "    corpus.append(tokens)\n",
        "\n",
        "  return corpus"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8xi4sUEGiIP"
      },
      "source": [
        "# def remove_notalpha"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHoIl5oRGgL4"
      },
      "source": [
        "def remove_notalpha(texts):\n",
        "  nlp = spacy.load('en')\n",
        "  tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
        "  corpus = []\n",
        "  for text in texts:\n",
        "    tokens = tokenizer(text)\n",
        "    tokens = [token.text for token in tokens if token.is_alpha]\n",
        "    tokens = ' '.join(tokens)\n",
        "    corpus.append(tokens)\n",
        "\n",
        "  return corpus"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpfGCYJ1GBA_"
      },
      "source": [
        "# def remove_links"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vx9AT6a68VE_"
      },
      "source": [
        "def remove_links(texts):\n",
        "  nlp = spacy.load('en')\n",
        "  tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
        "  corpus = []\n",
        "  for text in texts:\n",
        "    tokens = tokenizer(text)\n",
        "    tokens = [token.text for token in tokens if not token.like_url]\n",
        "    tokens = ' '.join(tokens)\n",
        "    corpus.append(tokens)\n",
        "\n",
        "  return corpus"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxxS4zBpHu3o"
      },
      "source": [
        "# \"Pipelinizing\" the functions created previously"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr0AJNBUHM30"
      },
      "source": [
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "trans_concat = FunctionTransformer(concatenate)\n",
        "trans_lemma = FunctionTransformer(lemmatize)\n",
        "trans_stop = FunctionTransformer(remove_stopwords)\n",
        "trans_punct = FunctionTransformer(remove_punctuation)\n",
        "trans_alpha = FunctionTransformer(remove_notalpha)\n",
        "trans_links = FunctionTransformer(remove_links)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ48W1G7WD9n"
      },
      "source": [
        "dataset = pd.read_csv(\"train.csv\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e27D9z_FHM6T"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "transformers = [('concatenation', trans_concat),\n",
        "                ('lemmatization', trans_lemma),\n",
        "                ('links_removal', trans_links),\n",
        "                ('leave_alpha', trans_alpha),\n",
        "              #('stopwords_removal', trans_stop),\n",
        "              #('punctuation_removal', trans_punct)]\n",
        "              ]\n",
        "\n",
        "my_pipe = Pipeline(transformers)\n",
        "X = my_pipe.transform(dataset)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie6HkQJsI4qy"
      },
      "source": [
        "y = dataset['target']"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rqV344z5tS9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AeLwUaoVQjA"
      },
      "source": [
        "# Now we'll create TF-IFD vectorizer and Count vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMfWKvgWqyNz"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "max_features=1500\n",
        "tfidf = TfidfVectorizer(max_features=max_features)\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(X_train).toarray()\n",
        "X_test_tfidf = tfidf.transform(X_test).toarray()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hcmZPOj6S2D"
      },
      "source": [
        "f1_scores = {}"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgzDnZzj6Vvl"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB9o18-9HVy8"
      },
      "source": [
        "# lr_grid = {'penalty' : ['l1', 'l2'],\n",
        "#             'C' : np.logspace(-4, 4, 20),\n",
        "#             'solver' : ['liblinear'],\n",
        "#             'max_iter' : [10, 100, 500, 1000, 2000, 5000]}"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65ZORxv_HYIt"
      },
      "source": [
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# lr_model = LogisticRegression()\n",
        "# lr_model_grid = GridSearchCV(estimator=lr_model,\n",
        "#                              param_grid=lr_grid,\n",
        "#                              scoring='f1',\n",
        "#                              n_jobs=-1,\n",
        "#                              cv=5,\n",
        "#                              verbose=True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5CzwTppdc0e"
      },
      "source": [
        "# lr_model_grid.fit(X_train_tfidf, y_train)\n",
        "# lr_model_grid_params_tfidf = lr_model_grid.best_params_\n",
        "# print(lr_model_grid_params_tfidf)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bsoy2mS-jh4V"
      },
      "source": [
        "# Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n",
        "# [Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
        "# [Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    5.6s\n",
        "# [Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   18.5s\n",
        "# [Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:   39.8s\n",
        "# [Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed:  1.2min\n",
        "# [Parallel(n_jobs=-1)]: Done 1200 out of 1200 | elapsed:  3.4min finished\n",
        "# {'C': 1.623776739188721, 'max_iter': 10, 'penalty': 'l2', 'solver': 'liblinear'}"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTPzoqHMj1Bo"
      },
      "source": [
        "lr_model_grid_params_tfidf = {'C': 1.623776739188721,\n",
        "                              'max_iter': 10,\n",
        "                              'penalty': 'l2',\n",
        "                              'solver': 'liblinear'}"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xSv6OcdgLBL",
        "outputId": "46e4cbc7-a721-46fa-b204-380d10fdc8a5"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_model_tfidf = LogisticRegression(**lr_model_grid_params_tfidf)\n",
        "lr_model_tfidf.fit(X_train_tfidf, y_train)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
              "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
              "                   max_iter=10, multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFtmKStpHek3",
        "outputId": "6a97dad8-cf56-4558-d269-344d8f5559c3"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "y_pred_lr_tfidf = lr_model_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "f1_score_lr_tfidf = f1_score(y_test, y_pred_lr_tfidf)\n",
        "print(f1_score_lr_tfidf)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7363100252737994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUkNF5TaeZeN"
      },
      "source": [
        "# Using CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDCYTnOmfP8V"
      },
      "source": [
        "count = CountVectorizer(max_features=max_features)\n",
        "\n",
        "X_train_count = count.fit_transform(X_train).toarray()\n",
        "X_test_count = count.transform(X_test).toarray()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjs_LOrBHgdr"
      },
      "source": [
        "# lr_model_grid.fit(X_train_count, y_train)\n",
        "# lr_model_grid_params_count = lr_model_grid.best_params_\n",
        "# print(lr_model_grid_params_count)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdeZqVSbjoJt"
      },
      "source": [
        "# Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n",
        "# [Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
        "# [Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    5.0s\n",
        "# [Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   20.3s\n",
        "# [Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:   46.3s\n",
        "# [Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed:  1.5min\n",
        "# [Parallel(n_jobs=-1)]: Done 1200 out of 1200 | elapsed:  4.8min finished\n",
        "# {'C': 0.615848211066026, 'max_iter': 10, 'penalty': 'l2', 'solver': 'liblinear'}"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wv8Uq-ijtgb"
      },
      "source": [
        "lr_model_grid_params_count = {'C': 0.615848211066026,\n",
        "                              'max_iter': 10,\n",
        "                              'penalty': 'l2',\n",
        "                              'solver': 'liblinear'}"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ri7sSBoxeUm1",
        "outputId": "94d12d46-5564-46b3-f05c-c9410d9336c2"
      },
      "source": [
        "lr_model_count = LogisticRegression(**lr_model_grid_params_count)\n",
        "lr_model_count.fit(X_train_count, y_train)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.615848211066026, class_weight=None, dual=False,\n",
              "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
              "                   max_iter=10, multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eblHDiXeUpI",
        "outputId": "6eef55ab-80da-4ab2-c5e7-e7f2fad9b3fe"
      },
      "source": [
        "y_pred_lr_count = lr_model_count.predict(X_test_count)\n",
        "\n",
        "f1_score_lr_count = f1_score(y_test, y_pred_lr_count)\n",
        "print(f1_score_lr_count)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7417998317914214\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HT-M1xzl-EH"
      },
      "source": [
        "f1_scores[\"LogisticRegression(TFIDF)\"] = f1_score_lr_tfidf\n",
        "f1_scores[\"LogisticRegression(COUNT)\"] = f1_score_lr_count"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2_e5kFwuMp5",
        "outputId": "c3115631-1a35-45cb-dc1d-47a8ae3ef260"
      },
      "source": [
        "f1_scores"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'LogisticRegression(COUNT)': 0.7417998317914214,\n",
              " 'LogisticRegression(TFIDF)': 0.7363100252737994}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op7eTrJVl20A"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3p273t4tkMa3",
        "outputId": "7dc87858-0f67-4c7e-b365-69290fbc016d"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knc_model = KNeighborsClassifier()\n",
        "knc_model.fit(X_train_tfidf, y_train)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KYf-owXl5tq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70540811-b17f-4445-dea1-a40cfcd70296"
      },
      "source": [
        "y_pred_knc = knc_model.predict(X_test_tfidf)\n",
        "\n",
        "f1_score_knc = f1_score(y_test, y_pred_knc)\n",
        "print(f1_score_knc)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3951219512195122\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgUtGrdYv6Wm"
      },
      "source": [
        "# Decsision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEKDF9_k0ofh"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "dtc_grid = {\"max_depth\": [3, None],\n",
        "              \"max_features\": range(1, 10),\n",
        "              \"min_samples_leaf\": range(1, 10),\n",
        "              \"criterion\": [\"gini\", \"entropy\"]}\n",
        "\n",
        "dtc_search = GridSearchCV(estimator=DecisionTreeClassifier(),\n",
        "                          param_grid=dtc_grid,\n",
        "                          scoring='f1',\n",
        "                          n_jobs=-1,\n",
        "                          cv=5,\n",
        "                          verbose=True)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfDtJUqQ0tW3",
        "outputId": "b34b4199-19b2-4981-b927-8cac7df433a0"
      },
      "source": [
        "dtc_search.fit(X_train_tfidf, y_train)\n",
        "dtc_best_params_tfidf = dtc_search.best_params_\n",
        "print(dtc_best_params_tfidf)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 116 tasks      | elapsed:    4.7s\n",
            "[Parallel(n_jobs=-1)]: Done 716 tasks      | elapsed:   26.7s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'criterion': 'entropy', 'max_depth': None, 'max_features': 9, 'min_samples_leaf': 3}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 1620 out of 1620 | elapsed:  1.0min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ve0Fsz-xv1tX",
        "outputId": "59b0282e-d54f-4d4e-cd89-1d2a4ad4d383"
      },
      "source": [
        "dtc_model_tfidf = DecisionTreeClassifier(**dtc_best_params_tfidf)\n",
        "dtc_model_tfidf.fit(X_train_tfidf, y_train)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
              "                       max_depth=None, max_features=9, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=3, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSAWUHY6v9UE",
        "outputId": "21d64d2a-923f-4f86-9867-cc03553ac8ed"
      },
      "source": [
        "y_pred_dtc_tfidf = dtc_model_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "f1_score_dtc_tfidf = f1_score(y_test, y_pred_dtc_tfidf)\n",
        "print(f1_score_dtc_tfidf)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6526492851135408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncG-ITIdv_w_"
      },
      "source": [
        "f1_scores[\"DecisionTreeClassifier(TFIDF)\"] = f1_score_dtc_tfidf"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWjVxAEd2xhI"
      },
      "source": [
        "# Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeiySpy5waQr"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rfc_grid = {\"n_estimators\": [100, 200, 500, 1000],\n",
        "            \"max_depth\": [8, 15, 25, 30],\n",
        "            \"max_features\": [\"auto\", \"sqrt\"]}\n",
        "\n",
        "rfc_search = GridSearchCV(estimator=RandomForestClassifier(),\n",
        "                          param_grid=rfc_grid,\n",
        "                          scoring='f1',\n",
        "                          n_jobs=-1,\n",
        "                          cv=5,\n",
        "                          verbose=True)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdHWGAIq8_B6",
        "outputId": "ff4002a0-468c-4848-cc88-1dcc1c2b6e90"
      },
      "source": [
        "rfc_search.fit(X_train_tfidf, y_train)\n",
        "rfc_best_params_tfidf = rfc_search.best_params_\n",
        "print(rfc_best_params_tfidf)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  2.4min\n",
            "[Parallel(n_jobs=-1)]: Done 160 out of 160 | elapsed: 16.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'max_depth': 30, 'max_features': 'auto', 'n_estimators': 1000}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6pv6KqZ9UfL",
        "outputId": "eca13083-0baa-436b-819c-db235d50ad5a"
      },
      "source": [
        "rfc_model_tfidf = RandomForestClassifier(**rfc_best_params_tfidf)\n",
        "rfc_model_tfidf.fit(X_train_tfidf, y_train)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=30, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDk6I1PA9eX1",
        "outputId": "42857a9d-8951-4d54-e70e-dd84df242089"
      },
      "source": [
        "y_pred_rfc_tfidf = rfc_model_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "f1_score_rfc_tfidf = f1_score(y_test, y_pred_rfc_tfidf)\n",
        "print(f1_score_rfc_tfidf)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.634765625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-lwOjWt9iAD"
      },
      "source": [
        "f1_scores[\"RandomForestClassifier(TFIDF)\"] = f1_score_rfc_tfidf"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhE0mm3c-SuX",
        "outputId": "45073070-da0a-4841-b640-ba5a6df0919c"
      },
      "source": [
        "f1_scores"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DecisionTreeClassifier(TFIDF)': 0.6526492851135408,\n",
              " 'LogisticRegression(COUNT)': 0.7417998317914214,\n",
              " 'LogisticRegression(TFIDF)': 0.7363100252737994,\n",
              " 'RandomForestClassifier(TFIDF)': 0.634765625}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXeQywTCzyiJ"
      },
      "source": [
        "# Submission of model, trained on full training set, using TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHj79noy0EUu"
      },
      "source": [
        "train = pd.read_csv('train.csv')\r\n",
        "test = pd.read_csv('test.csv')"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfiDLbPG14uh"
      },
      "source": [
        "y = train['target']\r\n",
        "train.drop(['target'], axis=1, inplace=True)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ1HYRsz0Qpu"
      },
      "source": [
        "X_train_full = my_pipe.transform(train)\r\n",
        "X_test_full = my_pipe.transform(test)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HorDGW1t8iPN"
      },
      "source": [
        "max_features=2500\r\n",
        "tfidf_final = TfidfVectorizer(max_features=max_features)\r\n",
        "\r\n",
        "X_train_full = tfidf.fit_transform(X_train_full).toarray()\r\n",
        "X_test_full = tfidf.transform(X_test_full).toarray()"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sm9PKI-o1Thg",
        "outputId": "df4dc73d-5052-441d-8fcc-a7895131f684"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\r\n",
        "\r\n",
        "lr_model_final = LogisticRegression(**lr_model_grid_params_tfidf)\r\n",
        "lr_model_final.fit(X_train_full, y)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
              "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
              "                   max_iter=10, multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oacq2h9V3ns7"
      },
      "source": [
        "y_pred = lr_model_final.predict(X_test_full)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtcBRbS19GQl",
        "outputId": "bc8ef480-5bf3-499c-94b3-4a41892e7e9a"
      },
      "source": [
        "y_pred[:30]"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7TIfBFq9M3o"
      },
      "source": [
        "# Save test predictions to file\r\n",
        "output = pd.DataFrame({'id': test.id,\r\n",
        "                       'target': y_pred})\r\n",
        "output.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL0WVA7Y-yWN"
      },
      "source": [
        "Score on kaggle - 0.79374"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzAlPUwA-rVZ"
      },
      "source": [
        "# Submission of model, trained on full training set, using Count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbJLTsJO-3Yw"
      },
      "source": [
        "train = pd.read_csv('train.csv')\r\n",
        "test = pd.read_csv('test.csv')"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nVpOn7c-48p"
      },
      "source": [
        "y = train['target']\r\n",
        "train.drop(['target'], axis=1, inplace=True)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9khxVn6-4_H"
      },
      "source": [
        "X_train_full = my_pipe.transform(train)\r\n",
        "X_test_full = my_pipe.transform(test)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3quL7Jc9QRe"
      },
      "source": [
        "max_features = 2500\r\n",
        "count_final = CountVectorizer(max_features=max_features)\r\n",
        "\r\n",
        "X_train_full_count = count.fit_transform(X_train_full).toarray()\r\n",
        "X_test_full_count = count.transform(X_test_full).toarray()"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsiEZnIi_Qyw",
        "outputId": "ea47454a-1f5a-49cd-bfa6-19a2471900bb"
      },
      "source": [
        "lr_model_count_final = LogisticRegression(**lr_model_grid_params_tfidf)\r\n",
        "lr_model_count_final.fit(X_train_full_count, y)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.623776739188721, class_weight=None, dual=False,\n",
              "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
              "                   max_iter=10, multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV2_tLXq_mLH"
      },
      "source": [
        "y_pred_count = lr_model_count_final.predict(X_test_full_count)"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6HGCrtn_smz"
      },
      "source": [
        "# Save test predictions to file\r\n",
        "output = pd.DataFrame({'id': test.id,\r\n",
        "                       'target': y_pred_count})\r\n",
        "output.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBMnXKdTAYOM"
      },
      "source": [
        "Score on kaggle - 0.79374"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO-75UKPAYrD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}